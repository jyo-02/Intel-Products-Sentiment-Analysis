{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the links of techical reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import Firefox\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'processors.txt'\n",
    "\n",
    "# Read processor names from file\n",
    "with open(filename, 'r') as file:\n",
    "    processor_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "driver = Firefox()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Test search query\n",
    "test_query = 'test query'\n",
    "try:\n",
    "    driver.get(f'https://www.google.com/search?q={test_query}')\n",
    "    \n",
    "    # Wait for search results to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h3')))\n",
    "    \n",
    "    print(\"Google search test successful.\")\n",
    "except Exception as ex:\n",
    "    print(f\"Error during Google search test: {ex}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Define websites to search\n",
    "websites = {\n",
    "    'tomshardware': 'site:tomshardware.com',\n",
    "    'pcmag': 'site:pcmag.com'\n",
    "}\n",
    "\n",
    "for processor_name in processor_names:\n",
    "    processor_name_cleaned = processor_name.replace(' ', '+')\n",
    "    \n",
    "    for site, site_keyword in websites.items():\n",
    "        search_query = f'{processor_name_cleaned}+{site_keyword}+review'\n",
    "        try:\n",
    "            driver.get(f'https://www.google.com/search?q={search_query}')\n",
    "            \n",
    "            # Wait for search results to load\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h3')))\n",
    "            \n",
    "            # Find the first search result link\n",
    "            search_results = driver.find_elements(By.CSS_SELECTOR, 'h3')\n",
    "            first_result = search_results[0]\n",
    "            link = first_result.find_element(By.XPATH, './../../a').get_attribute('href')\n",
    "            \n",
    "            results.append({\n",
    "                'Processor': processor_name,\n",
    "                'Source': site,\n",
    "                'URL': link\n",
    "            })\n",
    "            \n",
    "            print(f\"Found {site} review for {processor_name}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Error processing {site} review for {processor_name}: {ex}\")\n",
    "    \n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('processors_links.csv', index=False)\n",
    "\n",
    "driver.quit()\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of techical reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import Firefox\n",
    "import time\n",
    "\n",
    "# Initialize Selenium WebDriver for Firefox\n",
    "driver = Firefox()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Function to scrape content from tomshardware.com\n",
    "def scrape_tomshardware(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(15)  # Wait for the page to load completely\n",
    "    main_content = driver.find_element(By.ID, 'article-body')\n",
    "    # Extract the paragraphs that are not inside figure or aside tags and do not contain a tags\n",
    "    paragraphs = main_content.find_elements(By.XPATH, './/p[not(ancestor::figure) and not(ancestor::aside) and not(descendant::a)]')\n",
    "    # Join the text content of the paragraphs\n",
    "    content = ' '.join([para.text for para in paragraphs])\n",
    "    return content\n",
    "\n",
    "# Function to scrape content from pcgamer.com\n",
    "def scrape_pcmag(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for the page to load completely\n",
    "    content_container = driver.find_element(By.ID, 'article')  # Example CSS selector\n",
    "    paragraphs = content_container.find_elements(By.TAG_NAME, 'p')\n",
    "    content = ' '.join([para.text for para in paragraphs])\n",
    "    return content\n",
    "\n",
    "# Read input CSV file\n",
    "input_csv = 'processors_links.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Prepare list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    processor = row['Processor']\n",
    "    source = row['Source']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Determine which scraping function to call based on the source\n",
    "    if source == 'tomshardware':\n",
    "        content = scrape_tomshardware(url)\n",
    "    elif source == 'pcmag':\n",
    "        content = scrape_pcmag(url)\n",
    "    else:\n",
    "        content = f\"Scraping function not defined for source: {source}\"\n",
    "    \n",
    "    results.append({'Processor': processor, 'Source': source, 'Content': content})\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_csv = 'scraped_processors.csv'\n",
    "results_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Scraping completed. Data saved to {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
